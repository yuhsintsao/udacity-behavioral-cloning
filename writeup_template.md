# **Behavioral Cloning** 

## Writeup

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report

[//]: # (Image References)

[video]:  ./run_best.mp4 "Autonomous Mode"
[image1]: ./examples/center.jpg "Center Camera Image"
[image2]: ./examples/left.jpg "Left Camera Image"
[image3]: ./examples/right.jpg "Right Camera Image"
[image4]: ./examples/center_flip.jpg "Flipped Image"

## Executive Summery 

In my view, the key part of this project is to build a nicer architecture of convolution neural network. Based on NVIDIA's model design, I came out with my model by making a few adjustments. I will give more details in the 'Rubric Points' section.

Unlike the previous two projects, which using real-world data collected through real cameras, the data (images) generated by a simulator is extremely homogeneous, i.e. the images of a same scene should share similar pixel values. Thus, I skipped the step of image pre-processing (e.g. applying grayscale, gaussian smooth, and histogram equalization) which plays a key part in the previous two projects. I only adopted cropping and normalization to the original data. 

Adding more data is extremely crucial to achieving a generalized model. However, it's better not to use the dataset recorded only several drivers' driving behaviors. The consistency in one driver's driving behavior helps make a trained model work smoothly with little effort. The above reflection is the lession I learned from my experiment of combining several data collected from different 'drivers'. Considering the limitation of my hardware equipment, I used only the example data to train my model.

The overall performance of my model is acceptable. Sometimes the car would be very close to the road line of the side, but it still keeps on the road track and would steer itself to the center of the road soon after.

## Video Display

![Autonomous][video]

## Image Display

![Center][image1]
![Left][image2]
![Right][image3]

![Flip Image][image4]

## Rubric Points
### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model\_best.h5 containing a trained convolution neural network 
* writeup_report.md or writeup\_report.pdf summarizing the results

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable

The model\_best.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model consists of a convolution neural network with both 5x5 and 3x3 filter sizes and depths between 24 and 64 (model\_best.py 'Model Design' lines 23-37) 

The model includes RELU layers to introduce nonlinearity, and the data is normalized in the model using a Keras lambda layer (code line 24). 

#### 2. Attempts to reduce overfitting in the model

The model contains dropout layers in order to reduce overfitting (model\_best.py 'Model Design' lines 21). 

The model was trained and validated on different data sets to ensure that the model was not overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually (model\_best.py 'Model Design' line 43).

#### 4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road. I used a combination of center lane driving, recovering from the left and right sides of the road. Moreover, I flipped the image to expend the dataset (model\_best.py 'Data Collection').

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

The overall strategy for deriving a model architecture was to add more convolution layers.

My first step was to use NVIDIA's model. I thought this model might be appropriate because it could already achieve a nicer outcome which is shown in the course video. Then I add two more convolution layers to the original model and I found a better performance of my model.

Then, I tuned the batch size. I set the size to 32 and 48, and I found the later a better choice. 32 is the default setting. I picked 48 because my final video is played at 48 fps. 

In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. To combat the overfitting, I modified the model by adding a maxpooling layer and a dropout layer. With the epoch set to 3, the gap between the training loss and the validation loss got smaller.  

The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle nearly crossed the lane line on the side. However, it never fell off the track. In sum, the vehicle is able to drive autonomously around the track without leaving the road.

#### 2. Final Model Architecture

The final model architecture (model.py lines 18-24) consisted of a convolution neural network with the following layers and layer sizes:

1st  convolution layer 5x5 filter 2x2 stride - activation function 'relu'
2nd  convolution layer 5x5 filter 2x2 stride - activation function 'relu'
<font color=red>3rd  convolution layer 5x5 filter 1x1 stride - activation function 'relu'</font> 
4th  convolution layer 3x3 filter 1x1 stride - activation function 'relu' 
<font color=red>5th  convolution layer 3x3 filter 1x1 stride - activation function 'relu'</font>
<font color=red>6th  maxpooling  layer</font>
7th  dense       layer
<font color=red>8th  dropout     layer</font>
9th  dense       layer
10th dense       layer
11th output      layer

ps: The parts of design different from NVIDIA's are highlighted in <font color=red>red</font>.

#### 3. Creation of the Training Set & Training Process

To begin with, I use the example data only. To augment the data sat, I also flipped images and angles.

After the collection process, I had 38572 number of data points. I then preprocessed these data by cropping and normalization. 

Then, I randomly shuffled the data set and put 20% of the data into a validation set. 

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was 3 as evidenced by the gap between the training loss and validation loss. I used an adam optimizer so that manually training the learning rate wasn't necessary.

I have recorded several laps on track one (folder: data\_new), but it turns out it's better not to combine two dataset representing different drives' behaviors. 
